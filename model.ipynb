{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Behavioral cloning project\n",
    "\n",
    "The objective of this project is to create a model with keras thar allows autonoumous driving in a simulator.\n",
    "You can find the video of the simulation as run.mp4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code\n",
    "\n",
    "This code uses keras version 2.0.4 and tensorflow 1.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import cv2\n",
    "import numpy as np\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Lambda\n",
    "from keras.layers import Conv2D, MaxPooling2D, Cropping2D, ELU\n",
    "from keras.regularizers import l2\n",
    "from keras.utils import np_utils\n",
    "from keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from keras.layers.noise import GaussianNoise\n",
    "\n",
    "from random import random, choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most important hiperparameters of the model is the correction factor. This parameter corrects the angle for left and right camera images. \n",
    "The current value is the best value that I found empirically for autonomous driving in the simulator. A bigger value and the car losses the track. If the value is smaller the loss result of the model is smaller too, but the car drives to straight and doesn't take well the curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correction_factor = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function creates the model for steering prediction. I started with a simple two layer neural network. Just to check everything is correct. Later I used a modified LeNet architecure, and the NVIDIA arquitecture. Because I wasn't getting good results with NVIDIA arquitecture I opted to modify it a bit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    ch, row, col = 3, 160, 320  # camera format\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Lambda(lambda x: x/127.5 - 1.,\n",
    "                     input_shape=(row, col, ch),\n",
    "                     output_shape=(row, col, ch)))\n",
    "    model.add(Cropping2D(cropping=((70, 25), (0, 0))))\n",
    "    model.add(Conv2D(24, (5, 5), strides=(2, 2), padding=\"same\"))\n",
    "    model.add(ELU())\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Conv2D(36, (5, 5), strides=(2, 2), padding=\"same\"))\n",
    "    model.add(ELU())\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Conv2D(48, (5, 5), strides=(2, 2), padding=\"same\"))\n",
    "    model.add(ELU())\n",
    "    #model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), strides=(1, 1), padding=\"same\"))\n",
    "    model.add(ELU())\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Conv2D(64, (3, 3), strides=(1, 1), padding=\"same\"))\n",
    "    model.add(ELU())\n",
    "    #model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(.2))\n",
    "    model.add(Dense(512))\n",
    "    model.add(Dropout(.5))\n",
    "    model.add(ELU())\n",
    "    model.add(Dense(128))\n",
    "    model.add(Dropout(.5))\n",
    "    model.add(ELU())\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use change bright function to randomize data and as data augmentation technic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def change_bright(img):\n",
    "    temp = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "    # Compute a random brightness value and apply to the image\n",
    "    brightness = .25 + np.random.uniform()\n",
    "    temp[:, :, 2] = temp[:, :, 2] * brightness\n",
    "    # Convert back to RGB and return\n",
    "    return cv2.cvtColor(temp, cv2.COLOR_HSV2RGB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the samples from the csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_samples():\n",
    "    with open('data/driving_log.csv') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        return list(reader)[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generator to create the training and validation data set from the csv samples. \n",
    "It randomizes the datain this way:\n",
    "- 50% of the times it flips the image left to right\n",
    "- 25% of the times it takes the left camera image, 25% the right, 50% the center one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generator(samples, batch_size=8):\n",
    "    num_samples = len(samples)\n",
    "    images = []\n",
    "    angles = []\n",
    "    while True: # Loop forever so the generator never terminates\n",
    "        samples = shuffle(samples)\n",
    "        for sample in samples:\n",
    "            i = choice([0,0,1,2])\n",
    "            name = './data/IMG/'+sample[i].split('/')[-1]\n",
    "            image = cv2.imread(name)\n",
    "            angle = float(sample[3])\n",
    "            image = change_bright(image)\n",
    "            if i==1:angle += correction_factor\n",
    "            if i==2:angle -= correction_factor\n",
    "            if random()>0.5:\n",
    "                images.append(image)\n",
    "                angles.append(angle)\n",
    "            else:\n",
    "                images.append(np.fliplr(image))\n",
    "                angles.append(-angle)\n",
    "            if len(images)>=batch_size:\n",
    "                X_train = np.array(images)\n",
    "                y_train = np.array(angles)\n",
    "                images=[]\n",
    "                angles=[]\n",
    "                yield shuffle(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to plot the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_history_object( history_object ):\n",
    "    ### print the keys contained in the history object\n",
    "    print(history_object.history.keys())\n",
    "    for i, loss, val_loss in zip(range(1,1+len(history_object.history['loss'])),history_object.history['loss'], history_object.history['val_loss']):\n",
    "        print(\"epoch\", i)\n",
    "        print(\"loss\", loss)\n",
    "        print(\"val_loss\", val_loss)\n",
    "\n",
    "    ### plot the training and validation loss for each epoch\n",
    "    plt.plot(history_object.history['loss'])\n",
    "    plt.plot(history_object.history['val_loss'])\n",
    "    plt.title('model mean squared error loss')\n",
    "    plt.ylabel('mean squared error loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['training set', 'validation set'], loc='upper right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the images in the dataset have a steering angle of 0. To improve my model I tried to make the steering angle distribution of the dataset more even. I tried many different ways but the driving results where worse.\n",
    "At the end all data is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_samples(samples):\n",
    "    rs = []\n",
    "    for sample in samples:\n",
    "        if float(sample[3])==0. and random()<=0.1:continue\n",
    "        rs.append(sample)\n",
    "    return rs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model function. I use a training set and a validation set. There is no test set because I use the driving simulation for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "   \n",
    "def train():\n",
    "    global correction_factor\n",
    "    print(\"Starting training\")\n",
    "    samples = load_samples()\n",
    "    #samples = clean_samples(samples)\n",
    "\n",
    "    train_samples, validation_samples = train_test_split(samples, test_size=0.1)\n",
    "    # compile and train the model using the generator function\n",
    "    train_generator = generator(train_samples)\n",
    "    validation_generator = generator(validation_samples)\n",
    "\n",
    "    # 7. Define model architecture\n",
    "    model = create_model()\n",
    "    print(\"Correction\", correction_factor)\n",
    "\n",
    "    # 9. Fit model on training data\n",
    "    history_object = model.fit_generator(train_generator, \n",
    "        verbose=1, \n",
    "        validation_steps=len(validation_samples), \n",
    "        epochs=1, \n",
    "        validation_data=validation_generator, \n",
    "        steps_per_epoch=len(train_samples)\n",
    "    )\n",
    " \n",
    "    model.save('model.h5')\n",
    "    #plot_history_object(history_object)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arquitecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lambda_1 (Lambda)            (None, 160, 320, 3)       0         \n",
      "_________________________________________________________________\n",
      "cropping2d_1 (Cropping2D)    (None, 65, 320, 3)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 33, 160, 24)       1824      \n",
      "_________________________________________________________________\n",
      "elu_1 (ELU)                  (None, 33, 160, 24)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 80, 24)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 8, 40, 36)         21636     \n",
      "_________________________________________________________________\n",
      "elu_2 (ELU)                  (None, 8, 40, 36)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 4, 20, 36)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 2, 10, 48)         43248     \n",
      "_________________________________________________________________\n",
      "elu_3 (ELU)                  (None, 2, 10, 48)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 2, 10, 64)         27712     \n",
      "_________________________________________________________________\n",
      "elu_4 (ELU)                  (None, 2, 10, 64)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 1, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 1, 5, 64)          36928     \n",
      "_________________________________________________________________\n",
      "elu_5 (ELU)                  (None, 1, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 320)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 320)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               164352    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "elu_6 (ELU)                  (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "elu_7 (ELU)                  (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 361,493\n",
      "Trainable params: 361,493\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "create_model().summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The arquitecture uses first normalization and a cropping layer to remove the horizon and bottom . It uses different conv nets with maxpooling and elu for non-linearity. At last there is a set of full connected layers with elu and dropout. The output is a regression layer of dimension 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution Design Approach\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final model arquitecture\n",
    "\n",
    "I finally opted for an arquitecture between LeNet and NVIDIA. At some point I remembered that it was more important the dataset than the model. With LeNet I was underfitting and with NVIDIA overfitting. \n",
    "To really improve the model I will focus in this tasks:\n",
    "- Automate the driving in the simulator. For the moment I have to do it manually and is very time consuming. I had many hiperparameters and arquitectures to test and it requires a lot effort.\n",
    "- With automatic testing and validation it would be easy to build a matrix with different arquitectures and hiperparameters. Then you can compare against the simulator. \n",
    "- Minimizing the loss function is not the best way to build the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of the training set\n",
    "\n",
    "I created my own training set. You can find it in my-data folder. I recorded three sets:\n",
    "- 1 clockwise staying in the center.\n",
    "- another one counter-clockwise staying in the center.\n",
    "- another one recovering from the sides to the center.\n",
    "\n",
    "Because my model was not working and I was not sure if the problem was in my model, in the data augmentation technics or my data quality I opted to use the udacity dataset to reduce sources of incertainity.\n",
    "\n",
    "### Data augmentation technics\n",
    "\n",
    "I tried many different technics to improve the dataset.\n",
    "\n",
    "#### Clean the data\n",
    "Because most of the steering angles are 0 and there are few examples with big angles. I tried to make the steering angle distribution more flat removing samples with angle 0. I got good loss result in the validation set, but a very poor result in the simulator.\n",
    "\n",
    "#### GaussianNoise\n",
    "I tried too to add a Gaussian Noise layer in my keras model. It's similar to add a dropout layer. In my model I didn't get good results and I tried other approach. With more time I can try more values to see if it improves, but my first approchimation is that it was no good\n",
    "\n",
    "#### Flip images\n",
    "I flip the images left to right and I got an improvement in the simulator.\n",
    "\n",
    "#### Change brightness\n",
    "I change the brightness of the images randomly and I got imporvement too.\n",
    "\n",
    "#### Use left and right camera images\n",
    "I use the left and right cameras with a correction factor. I got better loss function result with a correction factor around of 0.1, but better driving with 0.3. With lower values the car was not turning fast enough in the curves. With higher values the car losses control in straight road. The left and right cameras helps the car recover from the sides\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function and driving simulation\n",
    "\n",
    "To help me to tune my model I created a simple function to show me some predictions compared with the dataset.\n",
    "I was trying many different models and I realized that even when I was getting better loss function results the model performed much worse in the simulator. Losss function was not a good predictor of model performance. \n",
    "I closer inspection of the predictions show me that my models behave in this way:\n",
    "\n",
    "- The angle direction was correct. If the angle was positive my prediction was positive. If the angle was negative my prediction was negative.\n",
    "- The absolute value was almost always smaller. The steering angle of my prediction was smaller than the dataset. The result was that the car was getting out of track in the curves.\n",
    "\n",
    "My model had low angle prediction because in this way was reducing the loss. The problem is that we are not in an even problem. If the steering angle is too low the car goes out of track. If the steering angle is too big the car returns to the center. The driving will be a bit more rough but the car stays on the road.\n",
    "\n",
    "So the characteristics that I want from the predictions are:\n",
    "- Correct sign of the steering angle.\n",
    "- The magnitude of the angle has to be close or bigger that the dataset angle.\n",
    "- A smaller loss function doesn't imply a better model.\n",
    "\n",
    "Using the left and right camera images with the appropiate correction factor allowed me to adjust the model to fullfil this characteristics of the prediction. It's better to make worse predictions if you improve your safety."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "import h5py\n",
    "from model import generator, load_samples\n",
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def predict():\n",
    "\tmodel = load_model('model.h5')\n",
    "\tsamples = load_samples()\n",
    "\tdata = next(generator(samples, batch_size=32))\n",
    "\tX=[]\n",
    "\tY=[]\n",
    "\tfor x,y in zip(data[0], data[1]):\n",
    "\t\tprint(\"y:\",y)\n",
    "\t\tX.append(x)\n",
    "\t\tY.append(y)\n",
    "\tX = np.array(X)\n",
    "\tY = np.array(Y)\n",
    "\tpredictions = model.predict(X)\n",
    "\tfor prediction,y in zip(predictions,Y):\n",
    "\t\tprediction = prediction[0]\n",
    "\t\tprint(\"prediction:\",prediction, \"y\",y, \"diff:\",prediction-y)\n",
    "\tscore = model.evaluate(X,Y)\n",
    "\tprint(\"score\", score)\n",
    "    \n",
    "predict()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
